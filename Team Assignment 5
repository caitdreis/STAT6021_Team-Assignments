###########################
#                         #
#   Team Assignment 5     #
#                         #
###########################

## Please submit one set of answers per team.                    ##
## Your answers may be submitted as an annotated R file.         ##
## Please submit your plots in one PDF as a separate attachment. ##
###################################################################


#################
## Question 1: ##
#################

# For each part of this problem, you are to create (or find) a data set with about 30 observations 
# that is suitable for simple linear regression and satisfies the given specifications on the variance
# of the residuals. For each part, include a plot of your residuals that shows the required characteristic.

#   (a) The residuals have constant variance.

data <- read_csv("stat1.csv")

lm <- lm(y~x, data=data) #Run the linear regression

# generate predictions

Predicted_values = predict(lm, newdata = data)

# calculate the residuals
library("MASS") #need this package to calculate the studentized residual with studres()
studentized <- studres(lm); studentized #Find the studentized residuals
rstudent <-rstudent(lm); rstudent #Find the R-student residuals
standardized = rstandard(lm); standardized #Find the standardized residuals

# press residual
(r <- resid(lm))
(pr <- resid(lm)/(1 - lm.influence(lm)$hat))
sum(r^2)
press<- sum(pr^2) #PRESS residual is 321.22


# estimate against predicted values

plot(Predicted_values, standardized)# constant variance. It is hard to simulate data at the tail ends. But the values are very close
plot(Predicted_values, studentized) # constant variance. It is hard to simulate data at the tail ends. But the values are very close
plot(Predicted_values, rstudent) # constant variance. It is hard to simulate data at the tail ends. But the values are very close
plot(Predicted_values, pr) # constant variance. It is hard to simulate data at the tail ends. But the values are very close


#   (b) The residuals have variance proportional to E(y).

data <- read_csv("stat2.csv")

lm <- lm(y~x, data=data) #Run the linear regression

# generate predictions

Predicted_values = predict(lm, newdata = data)

# calculate the residuals
library("MASS") #need this package to calculate the studentized residual with studres()
studentized <- studres(lm); studentized #Find the studentized residuals
rstudent <-rstudent(lm); rstudent #Find the R-student residuals
standardized = rstandard(lm); standardized #Find the standardized residuals

# press residual
(r <- resid(lm))
(pr <- resid(lm)/(1 - lm.influence(lm)$hat))
sum(r^2)
press<- sum(pr^2) #PRESS residual is 321.22


# estimate against predicted values

plot(Predicted_values, standardized)# perfect graph. proportional to E(y)
plot(Predicted_values, studentized) # perfect graph. proportional to E(y)
plot(Predicted_values, rstudent) # perfect graph. proportional to E(y)
plot(Predicted_values, pr) # perfect graph. proportional to E(y)


#   (c) The residuals have variance proportional to E(y)^2.

data <- read_csv("stat3.csv")

lm <- lm(y~x, data=data) #Run the linear regression

# generate predictions

Predicted_values = predict(lm, newdata = data)

# calculate the residuals
library("MASS") #need this package to calculate the studentized residual with studres()
studentized <- studres(lm); studentized #Find the studentized residuals
rstudent <-rstudent(lm); rstudent #Find the R-student residuals
standardized = rstandard(lm); standardized #Find the standardized residuals

# press residual
(r <- resid(lm))
(pr <- resid(lm)/(1 - lm.influence(lm)$hat))
sum(r^2)
press<- sum(pr^2) #PRESS residual is 321.22


# estimate against predicted values

plot(Predicted_values, standardized)# reasonably E(y) square type of relationship
plot(Predicted_values, studentized)# reasonably E(y) square type of relationship
plot(Predicted_values, rstudent)# reasonably E(y) square type of relationship
plot(Predicted_values, pr)# reasonably E(y) square type of relationship




#   (d) The residuals have variance proportional to 1/E(y). 

data <- read_csv("stat4.csv")

lm <- lm(y~x, data=data) #Run the linear regression

# generate predictions

Predicted_values = predict(lm, newdata = data)

# calculate the residuals
library("MASS") #need this package to calculate the studentized residual with studres()
studentized <- studres(lm); studentized #Find the studentized residuals
rstudent <-rstudent(lm); rstudent #Find the R-student residuals
standardized = rstandard(lm); standardized #Find the standardized residuals

# press residual
(r <- resid(lm))
(pr <- resid(lm)/(1 - lm.influence(lm)$hat))
sum(r^2)
press<- sum(pr^2) #PRESS residual is 321.22


# estimate against predicted values

plot(Predicted_values, standardized) # inverse relationship
plot(Predicted_values, studentized) # inverse relationship
plot(Predicted_values, rstudent) # inverse relationship
plot(Predicted_values, pr) # inverse relationship



#   (e) The residuals have variance proportional to C-E(y) for some constant C.

data <- read_csv("stat5.csv")

lm <- lm(y~x, data=data) #Run the linear regression

# generate predictions

Predicted_values = predict(lm, newdata = data)

# calculate the residuals
library("MASS") #need this package to calculate the studentized residual with studres()
studentized <- studres(lm); studentized #Find the studentized residuals
rstudent <-rstudent(lm); rstudent #Find the R-student residuals
standardized = rstandard(lm); standardized #Find the standardized residuals

# press residual
(r <- resid(lm))
(pr <- resid(lm)/(1 - lm.influence(lm)$hat))
sum(r^2)
press<- sum(pr^2) #PRESS residual is 321.22


# estimate against predicted values

plot(Predicted_values, standardized)# C - E(y) type of relationship
plot(Predicted_values, studentized) # C - E(y) type of relationship
plot(Predicted_values, rstudent) # C - E(y) type of relationship
plot(Predicted_values, pr) # C - E(y) type of relationship


#   (f) The residuals have variance proportional to E(y)(C-E(y)) for some constant C.

data <- read_csv("stat6.csv")

lm <- lm(y~x, data=data) #Run the linear regression

# generate predictions

Predicted_values = predict(lm, newdata = data)

# calculate the residuals
library("MASS") #need this package to calculate the studentized residual with studres()
studentized <- studres(lm); studentized #Find the studentized residuals
rstudent <-rstudent(lm); rstudent #Find the R-student residuals
standardized = rstandard(lm); standardized #Find the standardized residuals

# press residual
(r <- resid(lm))
(pr <- resid(lm)/(1 - lm.influence(lm)$hat))
sum(r^2)
press<- sum(pr^2) #PRESS residual is 321.22


# estimate against predicted values

plot(Predicted_values, standardized)# variance proportional to y(c - y)
plot(Predicted_values, studentized) # variance proportional to y(c - y)
plot(Predicted_values, rstudent) # variance proportional to y(c - y)
plot(Predicted_values, pr) # variance proportional to y(c - y)


#################
## Question 2: ##
#################

# For each part of this problem do the following: 
#   (1) Use the data set you created or found for Problem 1 and run three separate regression models - ordinary 
#       least squares, transformed ordinary least squares, and weighted least squares.
#       (Depending on your data, you may need to estimate the weights for the weighted least squares. These can 
#        be found by using the predicted values (squared) of a regression model that uses the fitted values as the 
#        explanatory variable and the absolute residuals as the response variable.)
#   (2) Compare the results of the three models and indicate which seems to be the best model. 
# For each part, include at least three residual plot with at least one from each model.
#
#   (a) The residuals have variance proportional to E(y). # dataset B from above
data <- read.csv("stat2.csv")

lm <- lm(y~x, data=data) #ordinary least squares
summary(lm)
anova(lm) #MSE 34.9

## Box-Cox transformation
library(MASS)
boxcox(lm)
lambda.trans <- data$x[which.max(data$y)]

## Transform the response variable 
data$y_trans <- data$y^lambda.trans

## Run the transformed ordinary least squares linear regression
lm.trans <- lm(y_trans~x, data=data)
summary(lm.trans)
anova(lm.trans)
ti.trans <- rstudent(lm.trans); ti.trans

## Run the weighted linear regression
## Estimate weights 
data_low <- data[data$x<10,]
wt_low <- 1/summary(lm(y~x, data=data_low))$sigma^2
data_high <- data[data$x>=10,]
wt_high <- 1/summary(lm(y~x, data=data_high))$sigma^2
data$a <- ifelse(data$x<10,wt_low,wt_high)

lm.wt <- lm(y~x, data=data, weights=a)
summary(lm.wt)
anova(lm.wt) #MSE 1.000 
ei.wt <- weighted.residuals(lm.wt); ei.wt
ti.wt <- rstudent(lm.wt); ti.wt


#   (b) The residuals have variance proportional to E(y)^2. #dataset C from above

data.2.b <- read.csv("stat3.csv")

lm2 <- lm(y~x, data=data.2.b) #ordinary least squares
summary(lm2)
anova(lm2)

## Box-Cox transformation
library(MASS)
boxcox(lm2)
lambda.trans.2 <- data.2.b$x[which.max(data.2.b$y)]

## Transform the response variable 
data.2.b$y_trans <- data.2.b$y^lambda.trans.2

## Run the transformed ordinary least squares linear regression
lm.trans.2 <- lm(y_trans~x, data=data.2.b)
summary(lm.trans.2)
anova(lm.trans.2)
ti.trans.2 <- rstudent(lm.trans.2); ti.trans.2

## Run the weighted linear regression
## Estimate weights 
data_low.2 <- data.2.b[data.2.b$x<10,]
wt_low.2 <- 1/summary(lm(y~x, data=data_low.2))$sigma^2
data_high.2 <- data.2.b[data.2.b$x>=10,]
wt_high.2 <- 1/summary(lm(y~x, data=data_high.2))$sigma^2
data.2.b$a <- ifelse(data.2.b$x<10,wt_low.2,wt_high.2)

lm.wt.2 <- lm(y~x, data=data.2.b, weights=a)
summary(lm.wt.2)
anova(lm.wt.2) # MSE
ei.wt.2 <- weighted.residuals(lm.wt.2); ei.wt.2
ti.wt.2 <- rstudent(lm.wt.2); ti.wt.2

#   (c) The residuals have variance proportional to 1/E(y). #dataset D from above

data.2.c <- read.csv("stat4.csv")

lm3 <- lm(y~x, data=data.2.c) #ordinary least squares
summary(lm3)
anova(lm3)

## Box-Cox transformation
library(MASS)
boxcox(lm3)
lambda.trans.3 <- data.2.c$x[which.max(data.2.c$y)]

## Transform the response variable 
data.2.c$y_trans <- data.2.c$y^lambda.trans.3

## Run the transformed ordinary least squares linear regression
lm.trans.3 <- lm(y_trans~x, data=data.2.c)
summary(lm.trans.3)
anova(lm.trans.3)
ti.trans.3 <- rstudent(lm.trans.3); ti.trans.3

## Run the weighted linear regression
## Estimate weights 
data_low.3 <- data.2.c[data.2.c$x<10,]
wt_low.3 <- 1/summary(lm(y~x, data=data_low.3))$sigma^2
data_high.3 <- data.2.c[data.2.c$x>=10,]
wt_high.3 <- 1/summary(lm(y~x, data=data_high.3))$sigma^2
data.2.c$a <- ifelse(data.2.c$x<10,wt_low.3,wt_high.3)

lm.wt.3 <- lm(y~x, data=data.2.c, weights=a)
summary(lm.wt.3)
anova(lm.wt.3) # MSE
ei.wt.3 <- weighted.residuals(lm.wt.3); ei.wt.3
ti.wt.3 <- rstudent(lm.wt.3); ti.wt.3

#comparison of models

#residual plots for (a)

# weighted plot
rstudent <-rstudent(lm.wt); rstudent #Find the R-student residuals
plot(data$x,rstudent) ##rstudent residual plots vs. explanatory variables

# ordinal plot
rstudent <- rstudent(lm)
plot(data$x,rstudent)

# transformed plot
rstudent <- rstudent(lm.trans)
plot(data$x, rstudent)

#residual plots for (b)

# weighted plot
rstudent.2 <-rstudent(lm.wt.2); rstudent.2
plot(data.2.b$x, rstudent.2)

# ordinal plot
rstudent.2 <- rstudent(lm2)
plot(data.2.b$x, rstudent.2)

# transformed plot
rstudent.2 <- rstudent(lm.trans.2)
plot(data.2.b$x, rstudent.2)

#residual plots for (c)

# weighted plot
rstudent.3 <-rstudent(lm.wt.3); rstudent.3
plot(data.2.c$x, rstudent.3)

# ordinal plot
rstudent.3 <-rstudent(lm3); rstudent.3
plot(data.2.c$x, rstudent.3)

# transformed plot
rstudent.3 <-rstudent(lm.trans.3); rstudent.3
plot(data.2.c$x, rstudent.3)

# Overall, weighted model seems to be the best model.

